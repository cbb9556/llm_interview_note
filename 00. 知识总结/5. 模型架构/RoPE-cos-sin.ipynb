{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMz0E/MGyxSH3eAHbwegCzn",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aju22/RoPE-PyTorch/blob/main/RoPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/702114274"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Rotary Positional Embeddings (RoPE)**\n",
    "\n",
    "*Rotary Position Embedding (RoPE) is a technique used in transformer-based models to incorporate positional information into token representations. Unlike traditional positional encodings that rely on sine and cosine functions, RoPE utilizes rotation matrices to encode both absolute and relative positional information. This method was proposed as a way to enhance the effectiveness of positional embeddings in transformers.*"
   ],
   "metadata": {
    "id": "rOpRtQR3iQ3m"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **The Math**\n",
    "\n",
    "*Rotary encoding transforms pairs of features by rotating in the 2D plane. That is, it organizes the d features as d/2\n",
    "pairs. Each pair can be considered a coordinate in a 2D plane, and the encoding will rotate it by an angle depending on the position of the token.*\n",
    "\n",
    "$$\\text{Let } x_m^{(1)} \\text{ and } x_m^{(2)} \\text {be two features of the key or}\\\\\n",
    " \\text{query of any head at position m.}\\\\\n",
    " \\text{For simplicity assume x has only two features. Then the transformation is:}$$\n",
    "\n",
    "\n",
    "$$\\text{RoPE}(x_m^{(1)}, x_m^{(2)}, m) =\n",
    "\\begin{bmatrix}\n",
    "\\cos(m\\theta) & -\\sin(m\\theta) \\\\\n",
    "\\sin(m\\theta) & \\cos(m\\theta)\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_m^{(1)} \\\\\n",
    "x_m^{(2)}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "x_m^{(1)}\\cos(m\\theta) - x_m^{(2)}\\sin(m\\theta) \\\\\n",
    "x_m^{(2)}\\cos(m\\theta) + x_m^{(1)}\\sin(m\\theta)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$ \\Theta = \\theta_i = 10,000^{-\\frac{2(i-1)}{d}} , where ( i \\in [1, 2, ..., 2d] ) \\text{ for the ( 2d ) pairs of features.}$$"
   ],
   "metadata": {
    "id": "WyaC1bJYi5FH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **The Intuition**\n",
    "\n",
    "We would like to find a positional encoding function ***f(x,l)*** for ***x*** and its position ***l*** such that, for two items ***q*** and ***k*** and at positions ***m*** and ***n***, the innner product between ***f(q,m)*** and ***f(k,n)*** is sensitive only to the values ***q*** and ***k*** and their relative position ***m - n***.\n",
    "\n",
    " A key piece of information is the geometric definition of the dot product between Euclidean vectors:\n",
    "\n",
    " $$q \\cdot k = |q| |k| \\cos \\theta$$\n",
    "\n",
    " The RoPE embedding achieves this:\n",
    "\n",
    " \\begin{align}\n",
    "\\mathrm{RoPE}(x, m) &= xe^{mi\\theta} \\\\\n",
    "\\langle \\mathrm{RoPE}(q_j, m), \\mathrm{RoPE}(k_j, n)\\rangle &= \\langle q_j e^{mi\\theta}, k_j e^{ni\\theta} \\rangle \\\\\n",
    "&= q_j k_j e^{mi\\theta} \\overline{e^{ni\\theta}} \\\\\n",
    "&= q_j k_j e^{(m - n)i\\theta} \\\\\n",
    "&= \\mathrm{RoPE}(q_j k_j, m - n)\n",
    "\\end{align}\n",
    "\n"
   ],
   "metadata": {
    "id": "jxmdbvMAkX0T"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1]//2]\n",
    "    x2 = x[..., x.shape[-1]//2:]\n",
    "    return torch.cat([-x2,x1], dim=-1)\n",
    "\n",
    "def apply_rope(q, k, seq_len, dim, base = 10000, device = None):\n",
    "    self.dim = dim\n",
    "    self.base = base\n",
    "    inv_freq = 1.0 / (self.base ** (torch.arrage(0, self.dim, 2).float().to(device)/ self.dim))\n",
    "    t = torch.arrage(self.seq_len, device = device, dtype=inv_freq.dtype)\n",
    "    freqs = torch.outer(t, inv_freq)\n",
    "    emb = torch.cat([freqs, freqs], dim = -1)\n",
    "    cos = emb.cos()[:seq_len].to(x.dtype)\n",
    "    sin = emb.sin()[seq_len:].to(x.dtype)\n",
    "    q_em = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_em = (k * cos) + (rotate_half(q) * sin)\n",
    "    return q_em, k_em"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Qwen2\n",
    "class Qwen2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "\n",
    "        \"\"\" 这里即是在创建θ_i=10000^(−2i/d) \"\"\"\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        # # 生成一个从0到最大序列长度-1的张量，用于后续计算\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        \"\"\" 这里即是m*θ_i \"\"\"\n",
    "        freqs = torch.outer(t, self.inv_freq)  # 此处torch.outer等价于torch.matmul(t.unsqueeze(-1), self.inv_freq.unsqueeze(0))\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        \"\"\" 这里不是按照原文中说的[mθ_0, mθ_0, mθ_1, mθ_1, mθ_2, mθ_2, ...], 而是[mθ_0, mθ_1, mθ_2, ..., mθ_0, mθ_1, mθ_2...] \"\"\"\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "# Copied from transformers.models.llama.modeling_llama.rotate_half\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    \"\"\"\n",
    "    对输出进行一次“旋转”, 即式(31)中所示的向量[-q_1, q_0, -q_3, ....]\n",
    "    不过由于cos_sin_cache进行了修改, 这里也修改为了[-q_d//2, -q_d//2+1, ...., q_0, q_1, ...],\n",
    "    这种改变并不会影响式(30)的成立, 可以把Rm^T * Rn计算出来, 其实并没有变化\n",
    "    \"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    # 式(31)中所示\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    position_ids = torch.arange(past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device)  # 从past_key_values_length开始计算是因为KV Cache已经包含了位置编码\n",
    "    rotary_emb = Qwen2RotaryEmbedding(\n",
    "        self.head_dim,\n",
    "        max_position_embeddings=self.max_position_embeddings,\n",
    "        base=self.rope_theta,\n",
    "    )\n",
    "    cos, sin = rotary_emb(value_states, seq_len=seq_length + past_key_values_length)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "    # 后面就是正常计算attention weight了\n",
    "    # repeat k/v heads if n_kv_heads < n_heads\n",
    "    key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "\n",
    "  def __init__(self, d: int, base: int = 10_000):\n",
    "\n",
    "    super().__init__()\n",
    "    self.base = base\n",
    "    self.d = d\n",
    "    self.cos_cached = None\n",
    "    self.sin_cached = None\n",
    "\n",
    "  def _build_cache(self, x: torch.Tensor):\n",
    "\n",
    "    if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n",
    "      return\n",
    "\n",
    "    seq_len = x.shape[0]\n",
    "\n",
    "    theta = 1. / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device) # THETA = 10,000^(-2*i/d) or 1/10,000^(2i/d)\n",
    "\n",
    "    seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device) #Position Index -> [0,1,2...seq-1]\n",
    "\n",
    "    idx_theta = torch.einsum('n,d->nd', seq_idx, theta)  #Calculates m*(THETA) = [ [0, 0...], [THETA_1, THETA_2...THETA_d/2], ... [seq-1*(THETA_1), seq-1*(THETA_2)...] ]\n",
    "\n",
    "    idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1) # [THETA_1, THETA_2...THETA_d/2] -> [THETA_1, THETA_2...THETA_d]\n",
    "\n",
    "    self.cos_cached = idx_theta2.cos()[:, None, None, :] #Cache [cosTHETA_1, cosTHETA_2...cosTHETA_d]\n",
    "    self.sin_cached = idx_theta2.sin()[:, None, None, :] #cache [sinTHETA_1, sinTHETA_2...sinTHETA_d]\n",
    "\n",
    "  def _neg_half(self, x: torch.Tensor):\n",
    "\n",
    "    d_2 = self.d // 2 #\n",
    "\n",
    "    return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1) # [x_1, x_2,...x_d] -> [-x_d/2, ... -x_d, x_1, ... x_d/2]\n",
    "\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "\n",
    "    self._build_cache(x)\n",
    "\n",
    "    neg_half_x = self._neg_half(x)\n",
    "\n",
    "    x_rope = (x * self.cos_cached[:x.shape[0]]) + (neg_half_x * self.sin_cached[:x.shape[0]]) # [x_1*cosTHETA_1 - x_d/2*sinTHETA_d/2, ....]\n",
    "\n",
    "    return x_rope"
   ],
   "metadata": {
    "id": "XNeygwV2gEWH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "*We pair up the positive cosines and negative half sines to get the final embeddings as follows*:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_m^{(i)} \\\\\n",
    "x_m^{(i+d/2)}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "x_m^{(i)}\\cos(m\\theta_i) - x_m^{(i+d/2)}\\sin(m\\theta_i) \\\\\n",
    "x_m^{(i+d/2)}\\cos(m\\theta_i) + x_m^{(i)}\\sin(m\\theta_i)\n",
    "\\end{bmatrix}$$"
   ],
   "metadata": {
    "id": "BkZgSdWYhNeF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Test"
   ],
   "metadata": {
    "id": "i8FN5Z2YV6Z4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]], dtype=torch.float)\n",
    "x = x[:, None, None, :]"
   ],
   "metadata": {
    "id": "VCvQyvh_Txk-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "RotaryPositionalEmbeddings(4)(x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zcI4DomV5va",
    "outputId": "c73b6c19-af73-4dec-d096-1cae45e84c21"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[  1.0000,   2.0000,   3.0000,   4.0000]]],\n",
       "\n",
       "\n",
       "        [[[ -2.8876,   4.9298,   6.6077,   7.0496]]],\n",
       "\n",
       "\n",
       "        [[[-11.0967,   7.7984,   2.6198,  10.1580]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "RObvHOezWAkb"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
