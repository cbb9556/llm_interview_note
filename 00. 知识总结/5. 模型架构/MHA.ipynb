{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "1, qkv都是线性映射，输入和输出的dim都是 hidden size\n",
    "2， 需要将qkv进行分头，使用view将 x的形状变为， num-head 和 hidden-dim\n",
    "3， 注意力分数， qk进行张量乘，除以sqrt(hidden-dim)\n",
    "4，注意力分数，需要加上mask*-1e-9然后进行softmax归一化，得到注意力prob\n",
    "5，然后，注意力prob与v进行张量乘\n",
    "6，使用view进行形状变化，变得与x的形状相同，然后进行最后的输出线性转换"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class MutiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MutiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        assert (hidden_size % num_heads) == 0, \"hidden_size must be divisible by num_heads\"\n",
    "\n",
    "        ## 初始化Q、K、V投影矩阵\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        ## 输出线性层\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_state, attention_mask=None):\n",
    "        #hidden_state就是输入的 doc的句子，shape为 [batch_size, seq_len, hidden_size]\n",
    "        batch_size = hidden_state.size()[0]\n",
    "\n",
    "        query = self.q_linear(hidden_state)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "\n",
    "        query = self.split_head(query)\n",
    "        key = self.split_head(key)\n",
    "        # value shape为 [batch_size, num_heads, seq_len, hidden_dim]\n",
    "        value = self.split_head(value)\n",
    "\n",
    "        ## 计算注意力分数，-1 为 hidden_size,-2为seqlen\n",
    "        # 输出为 [batch_size, num_heads, seq_len, seq_len]\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "\n",
    "        if attention_mask != None:\n",
    "            attention_scores += attention_mask * -1e-9\n",
    "\n",
    "        ## 对注意力分数进行归一化，按列 进行softmax， 每一列数值范围0-1，和为1\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1) #解决ICS问题\n",
    "\n",
    "        # 输出output的形状为 [batch_size, num_heads, seq_len, hidden_dim]\n",
    "        output = torch.matmul(attention_probs, value)\n",
    "\n",
    "        ## 对注意力输出进行拼接，contiguous()确保内存连续性\n",
    "        # 输出形状为 [batch_size, seq_len, hidden_size]\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)\n",
    "\n",
    "        output = self.o_linear(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def split_head(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        # -1位置的dim，自动计算，代表seqlen\n",
    "        return x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T02:50:32.474226300Z",
     "start_time": "2025-03-27T02:50:28.676741700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module() takes at most 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mMHA\u001B[39;00m(nn\u001B[38;5;241m.\u001B[39mmodules):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, num_head, hidden_size):\n\u001B[0;32m      5\u001B[0m         \u001B[38;5;28msuper\u001B[39m(MHA,\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[1;31mTypeError\u001B[0m: module() takes at most 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MHA(nn.modules):\n",
    "    def __init__(self, num_head, hidden_size):\n",
    "        super(MHA,self).__init__()\n",
    "        self.num_head = num_head\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = hidden_size // num_head\n",
    "        self.qlinear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.klinear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.vlinear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.olinear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self,x, mask=None):\n",
    "\n",
    "        q = self.qlinear(x)\n",
    "        k = self.klinear(x)\n",
    "        v = self.vlinear(x)\n",
    "\n",
    "        q = self.split_head(q)\n",
    "        k = self.split_head(k)\n",
    "        v = self.split_head(v)\n",
    "\n",
    "        att_score = torch.matul(q, k.transpose(-1,-2)) // torch.sqrt(torch.tensor(self.head_dim))\n",
    "\n",
    "        if mask:\n",
    "            att_score += mask * -1e-9\n",
    "\n",
    "        att_prob = torch.softmax(att_score, dim = -1)\n",
    "        batch_size = x.size()[0]\n",
    "        output = torch.matul(att_prob, v).transpose(1,2).contiguous().view(batch_size, -1, self.num_head * self.hidde_dim)\n",
    "        output = self.olinear(output)\n",
    "        return output\n",
    "\n",
    "    def split_head(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        return x.view(batch_size, -1, self.num_head, self.hidden_size).transpose(1,2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T04:57:42.902526800Z",
     "start_time": "2025-03-27T04:57:42.668154200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "定义三个函数 forward、init、splithead\n",
    "\n",
    "forward函数，将输入x变化为融合 注意力的x, 也就是输入的x的形状和输出的x的形状要是一样，这样可以在多层的注意力模块传递\n",
    "    需要构造qkv，首先使用linear函数将x变成qkv的映射，对qkv的映射进行分头，对分头的qk进行注意力分数计算，需要将k转置需要d正则化，需要mask 乘以-1e-9然后softmax -1dim，然后matul乘以v的映射然后转置到与x形状相同，最后加线性层映射x为x形状\n",
    "分头函数，需要将 num——head 和 head-dim分头分出来，-1自动计算seqlen"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
