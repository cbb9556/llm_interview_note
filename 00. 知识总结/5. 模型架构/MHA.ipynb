{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "1, qkv都是线性映射，输入和输出的dim都是 hidden size\n",
    "2， 需要将qkv进行分头，使用view将 x的形状变为， num-head 和 hidden-dim\n",
    "3， 注意力分数， qk进行张量乘，除以sqrt(hidden-dim)\n",
    "4，注意力分数，需要加上mask*-1e-9然后进行softmax归一化，得到注意力prob\n",
    "5，然后，注意力prob与v进行张量乘\n",
    "6，使用view进行形状变化，变得与x的形状相同，然后进行最后的输出线性转换"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class MutiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MutiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        assert (hidden_size % num_heads) == 0, \"hidden_size must be divisible by num_heads\"\n",
    "\n",
    "        ## 初始化Q、K、V投影矩阵\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        ## 输出线性层\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_state, attention_mask=None):\n",
    "        #hidden_state就是输入的 doc的句子，shape为 [batch_size, seq_len, hidden_size]\n",
    "        batch_size = hidden_state.size()[0]\n",
    "\n",
    "        query = self.q_linear(hidden_state)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "\n",
    "        query = self.split_head(query)\n",
    "        key = self.split_head(key)\n",
    "        value = self.split_head(value)\n",
    "\n",
    "        ## 计算注意力分数，-1 为 hidden_size,-2为seqlen\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "\n",
    "        if attention_mask != None:\n",
    "            attention_scores += attention_mask * -1e-9\n",
    "\n",
    "        ## 对注意力分数进行归一化，按列 进行softmax\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_probs, value)\n",
    "\n",
    "        ## 对注意力输出进行拼接，contiguous()确保内存连续性\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)\n",
    "\n",
    "        output = self.o_linear(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def split_head(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        # -1位置的dim，自动计算，代表seqlen\n",
    "        return x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
