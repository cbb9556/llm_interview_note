{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "1， q需要分为多个head，kv只有1个head，其他和MHA相同"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class MutiQueryAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MutiQueryAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        ## 初始化Q、K、V投影矩阵\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, self.head_dim) ###\n",
    "        self.v_linear = nn.Linear(hidden_size, self.head_dim) ###\n",
    "\n",
    "        ## 输出线性层\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_state, attention_mask=None):\n",
    "        batch_size = hidden_state.size()[0]\n",
    "\n",
    "        query = self.q_linear(hidden_state)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "\n",
    "        query = self.split_head(query)\n",
    "        key = self.split_head(key, 1)\n",
    "        value = self.split_head(value, 1)\n",
    "\n",
    "        ## 计算注意力分数\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim)) #广播机制，k-v只有1个头，广播到多个头\n",
    "\n",
    "        if attention_mask != None:\n",
    "            attention_scores += attention_mask * -1e-9\n",
    "\n",
    "        ## 对注意力分数进行归一化\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_probs, value)\n",
    "\n",
    "        output = output.transpose(-1, -2).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)\n",
    "\n",
    "        output = self.o_linear(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def split_head(self, x, head_num=None):\n",
    "\n",
    "        batch_size = x.size()[0]\n",
    "\n",
    "        if head_num == None:\n",
    "            return x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2) # q有多个头\n",
    "        else:\n",
    "            return x.view(batch_size, -1, head_num, self.head_dim).transpose(1,2) # k-v只有1个头，"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-13T12:47:54.212464400Z",
     "start_time": "2025-02-13T12:47:45.330955500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
