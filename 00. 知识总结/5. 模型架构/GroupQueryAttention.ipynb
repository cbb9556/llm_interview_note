{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-07T05:00:28.191606600Z",
     "start_time": "2025-01-07T05:00:27.567276300Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mGroupQueryAttention\u001B[39;00m(\u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_size, head_num, group_num):\n\u001B[0;32m      3\u001B[0m         \u001B[38;5;28msuper\u001B[39m(GroupQueryAttention, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class GroupQueryAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, head_num, group_num):\n",
    "        super(GroupQueryAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_num = head_num\n",
    "        self.group_num = group_num\n",
    "        self.head_dim = self.hidden_size // head_num\n",
    "\n",
    "        self.q_linear = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = torch.nn.Linear(hidden_size, self.group_num * self.head_dim)\n",
    "        self.v_linear = torch.nn.Linear(hidden_size, self.group_num * self.head_dim)\n",
    "        self.o_linear = torch.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_state, mask=None):\n",
    "        batch_size = hidden_state.size()[0]\n",
    "\n",
    "        query = self.q_linear(hidden_state)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "\n",
    "        query = self.split_head(query)\n",
    "        key = self.split_head(key, self.group_num)\n",
    "        value = self.split_head(value, self.group_num)\n",
    "\n",
    "        atten_score = torch.matmul(query, key.transpose(-1,-2)) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "\n",
    "        if mask:\n",
    "            atten_score += mask*(-1e-9)\n",
    "\n",
    "        atten_score = torch.softmax(atten_score, dim=-1)\n",
    "        output = torch.matmul(atten_score, value)\n",
    "        output = output.transpose(-1,-2).contiguous().view(batch_size, -1, self.group_num*self.head_dim)\n",
    "\n",
    "        return self.o_linear(output)\n",
    "\n",
    "    def split_head(self, x, group_num):\n",
    "        batch_size, seq_len = x.size()[:2]\n",
    "\n",
    "        if group_num:\n",
    "            x = x.view(batch_size, -1, self.group_num, self.head_dim).transpose(1,2)\n",
    "            x = x[:,:,None,:,:].expand(batch_size, self.group_num, self.head_num//self.group_num, seq_len, self.head_dim).reshape(batch_size, self.head_num, seq_len, self.head_dim)\n",
    "        else:\n",
    "            x = x.view(batch_size, -1, self.head_num, self.head_dim).transpose(1,2)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalGroupQueryAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    实现了一个基于组查询的因果注意力机制模块。\n",
    "\n",
    "    该模块通过将隐藏状态线性变换为查询、键和值，然后在多个注意力头上进行组查询，\n",
    "    最后通过一个线性变换整合结果，输出最终的注意力结果。特别地，该模块实现了因果注意力，\n",
    "    确保每个位置只能关注到它之前的位置。\n",
    "\n",
    "    参数:\n",
    "    hidden_size (int): 隐藏状态的维度。\n",
    "    head_num (int): 注意力头的数量。\n",
    "    group_num (int): 查询组的数量。\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, head_num, group_num):\n",
    "        super(CausalGroupQueryAttention, self).__init__()\n",
    "        # 初始化隐藏状态维度、注意力头数量和查询组数量\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_num = head_num\n",
    "        self.group_num = group_num\n",
    "        # 计算每个注意力头的维度\n",
    "        self.head_dim = self.hidden_size // head_num\n",
    "\n",
    "        # 初始化线性变换层，用于生成查询、键、值和最终的输出整合\n",
    "        self.q_linear = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = torch.nn.Linear(hidden_size, self.group_num * self.head_dim)\n",
    "        self.v_linear = torch.nn.Linear(hidden_size, self.group_num * self.head_dim)\n",
    "        self.o_linear = torch.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_state, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数，实现了组查询因果注意力机制。\n",
    "\n",
    "        参数:\n",
    "        hidden_state (Tensor): 输入的隐藏状态，形状为[batch_size, seq_len, hidden_size]。\n",
    "        mask (Tensor, optional): 注意力掩码，用于指定某些位置的注意力得分为极小值，以避免在计算注意力时考虑这些位置。\n",
    "\n",
    "        返回:\n",
    "        Tensor: 注意力机制整合后的输出，形状为[batch_size, seq_len, hidden_size]。\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_state.size()\n",
    "\n",
    "        # 对隐藏状态进行线性变换，生成查询、键和值\n",
    "        query = self.q_linear(hidden_state)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "\n",
    "        # 将查询、键和值分割为多个注意力头\n",
    "        query = self.split_head(query)\n",
    "        key = self.split_head(key, self.group_num)\n",
    "        value = self.split_head(value, self.group_num)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        atten_score = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "\n",
    "        # 生成因果掩码，确保每个位置只能看到它之前的位置\n",
    "        causal_mask = torch.triu(torch.ones((seq_len, seq_len), device=hidden_state.device)).view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * -1e9\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :] + causal_mask\n",
    "        else:\n",
    "            mask = causal_mask\n",
    "\n",
    "        # 应用掩码atten_score的形状 batch_size, num_heads, seq_len, seq_len\n",
    "        # 这里mask通过广播机制，加到了atten_score上\n",
    "        atten_score += mask\n",
    "\n",
    "        # 计算注意力权重\n",
    "        atten_weights = torch.softmax(atten_score, dim=-1)\n",
    "\n",
    "        # 计算加权和\n",
    "        output = torch.matmul(atten_weights, value)\n",
    "        output = output.transpose(-1, -2).contiguous().view(batch_size, -1, self.group_num * self.head_dim)\n",
    "\n",
    "        return self.o_linear(output)\n",
    "\n",
    "    def split_head(self, x, group_num=None):\n",
    "        \"\"\"\n",
    "        将输入张量分割为多个注意力头。\n",
    "\n",
    "        参数:\n",
    "        x (Tensor): 输入张量，形状为[batch_size, seq_len, hidden_size]。\n",
    "        group_num (int, optional): 查询组的数量。\n",
    "\n",
    "        返回:\n",
    "        Tensor: 分割后的张量，形状为[batch_size, head_num, seq_len, head_dim]。\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.size()[:2]\n",
    "\n",
    "        if group_num is not None:\n",
    "            x = x.view(batch_size, -1, group_num, self.head_dim).transpose(1, 2)\n",
    "            x = x[:, :, None, :, :].expand(batch_size, group_num, self.head_num // group_num, seq_len, self.head_dim).reshape(batch_size, self.head_num, seq_len, self.head_dim)\n",
    "        else:\n",
    "            x = x.view(batch_size, -1, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
