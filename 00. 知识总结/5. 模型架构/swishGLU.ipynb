{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class GLUFFN(nn.Modules):\n",
    "    def __init__(self, h1, h2, h3, hiddensize):\n",
    "        super(GLUFFN, self).__init__()\n",
    "        self.w1 = nn.Linear(hiddensize, h1)\n",
    "        self.w2 = nn.Linear(hiddensize, h2)\n",
    "        self.w3 = nn.Linear(hiddensize, h3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.w1(x)\n",
    "        x2 = self.w2(x)\n",
    "        x2 = torch.matmul(x2, torch.sigmoid(x2))\n",
    "        x3 = self.w3(torch.matmul(x1, x2))\n",
    "\n",
    "        return x3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "以下是 SwiGLU FFN（基于Swish门控的前馈网络）‌ 的完整 PyTorch 实现代码，结合了 Transformer 中常用的架构设计（类似 LLaMA 和 PaLM 的实现方式）：\n",
    "\n",
    "# 完整代码实现‌\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "         super().__init__()\n",
    "        # SwiGLU 核心组件\n",
    "        self.w1 = nn.Linear(d_model, d_ff)    # 门控分支的线性变换\n",
    "        self.w2 = nn.Linear(d_model, d_ff)    # 值分支的线性变换\n",
    "        self.w3 = nn.Linear(d_ff, d_model)     # 输出投影层\n",
    "        self.dropout = nn.Dropout(dropout)    # 可选Dropout\n",
    "\n",
    "        # 初始化参数（类似LLaMA的初始化方式）\n",
    "        nn.init.normal_(self.w1.weight, std=0.02)\n",
    "        nn.init.normal_(self.w2.weight, std=0.02)\n",
    "        nn.init.normal_(self.w3.weight, std=0.02)\n",
    "        nn.init.zeros_(self.w1.bias)\n",
    "        nn.init.zeros_(self.w2.bias)\n",
    "        nn.init.zeros_(self.w3.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # SwiGLU门控计算\n",
    "        gate = F.silu(self.w1(x))    # Swish(β=1) = SiLU\n",
    "        value = self.w2(x)\n",
    "        x = gate * value             # 门控融合\n",
    "\n",
    "        # 输出投影 + Dropout\n",
    "        return self.dropout(self.w3(x))\n",
    "\n",
    "代码解析‌\n",
    "\n",
    "架构设计‌：\n",
    "\n",
    "输入/输出维度‌：d_model 是 Transformer 的隐藏层维度（例如 512），d_ff 是 FFN 的中间扩展维度（例如 2048）。\n",
    "三线性层设计‌：\n",
    "w1 和 w2 分别生成门控和值的中间表示。\n",
    "w3 将 d_ff 维的特征投影回 d_model 维，保持输入输出维度一致。\n",
    "参数初始化‌：使用小标准差正态分布初始化权重，偏置初始化为零（与大模型训练稳定性相关）。\n",
    "\n",
    "其中 Swish 使用 PyTorch 内置的 F.silu 实现。门控融合后通过 W3 投影回原始维度。Dropout‌：可选丢弃层防止过拟合。\n",
    "\n",
    "使用示例‌\n",
    "python\n",
    "Copy Code\n",
    "# 定义输入 (batch_size=2, seq_len=10, d_model=512)\n",
    "x = torch.randn(2, 10, 512)\n",
    "\n",
    "# 初始化SwiGLU FFN (扩展维度d_ff=2048)\n",
    "ffn = SwiGLUFFN(d_model=512, d_ff=2048)\n",
    "\n",
    "# 前向传播\n",
    "output = ffn(x)\n",
    "\n",
    "# 验证输入输出维度一致\n",
    "print(output.shape)  # torch.Size([2, 10, 512])\n",
    "\n",
    "扩展：集成到Transformer Block‌\n",
    "python\n",
    "Copy Code\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, n_heads)\n",
    "        self.ffn = SwiGLUFFN(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 自注意力残差连接\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # FFN残差连接\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = x + ffn_out\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "关键优势‌\n",
    "更强的非线性‌：Swish门控比传统ReLU能捕获更复杂的特征交互。\n",
    "训练稳定性‌：门控机制动态调整特征权重，缓解梯度消失。\n",
    "兼容性‌：可直接替换标准Transformer中的FFN层（参数规模约为标准FFN的 2/3，需调整 d_ff 保持计算量一致）。\n",
    "\n",
    "如需调整模型规模，只需修改 d_model 和 d_ff 的比例（例如 LLaMA-7B 使用 d_ff = 2.7 * d_model）。"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
