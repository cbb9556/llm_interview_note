{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "见5模型架构图， 黄色和红色是k-v，那么group_num就是k-v的数量，group_num=1那么就是，1组维度为hidden_dim的k-v，共享多头维度为hidden_dim的q\n",
    "\n",
    "核心就是,\n",
    "（1）需要共享的k-v为：group_num个，每个都是hidden_dim的维度。需要在linear中，将k-v的维度变为group_num*head_dim，\n",
    "（2）用于共享，需要复制k-v次数为num_head//group_num:expend(batch_size, group_num, self.num_head//group_num, seq_len, self.head_dim).reshape(batch_size, self.num_head//group_num*group_num, seq_len, self.head_dim)\n",
    "\n",
    "其他与 MHA完全相同"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mGroupQueryAttention\u001B[39;00m(\u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_size, head_num, group_num):\n\u001B[0;32m      3\u001B[0m         \u001B[38;5;28msuper\u001B[39m(GroupQueryAttention, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class GroupQueryAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, group_num):\n",
    "        super(GroupQueryAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.group_num = group_num\n",
    "\n",
    "        ## 初始化Q、K、V投影矩阵\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, self.group_num * self.head_dim) # 每个组有group_num个q。每个q有head_size dim\n",
    "        self.v_linear = nn.Linear(hidden_size, self.group_num * self.head_dim) # 每个k-v 处理 group_num个\n",
    "\n",
    "        ## 输出线性层\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_state, attention_mask=None):\n",
    "       #hidden_state就是输入的 训练doc句子，shape为 [batch_size, seq_len, hidden_size]\n",
    "        batch_size = hidden_state.size()[0]\n",
    "\n",
    "        query = self.q_linear(hidden_state)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "\n",
    "        query = self.split_head(query)\n",
    "        key = self.split_head(key, self.group_num)\n",
    "        value = self.split_head(value, self.group_num)\n",
    "\n",
    "        # 注意力分数的最后两维度 就是 seq_len * seq_len，因为，q-k最后两维为 seq_len * head_dim，而softmax是对每个seq位置进行\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "\n",
    "        if attention_mask != None:\n",
    "            attention_scores += attention_mask * -1e-9 # 利用 softmax函数的饱和性，实现mask机制\n",
    "\n",
    "        # atten_probs的形状为 batch_size, num_heads, seq_len, seq_len。最后一维代表的 就是每个位置 对于其他位置的 注意力权重。乘以v，就是最终的结果\n",
    "        # softmax对 seq_len * seq_len 矩阵的每个位置都softmax激活。包括mask住的位置，softmax后为0。softmax后矩阵仍然为 seq_len * seq_len，再去与 V相乘变为，seq_len * head_dim\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "       #output的形状为 batch_size, num_heads, seq_len, head_dim\n",
    "        output = torch.matmul(attention_probs, value)\n",
    "\n",
    "       # 将多头 合并\n",
    "        output = output.transpose(-1, -2).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)\n",
    "\n",
    "       # 增加线性投影\n",
    "        output = self.o_linear(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    # num_heads 个头 每个头 head_dim维度， 共享group_num个k-v每个k-v 的维度为head_dim，比如：8个头每个头768//8维度，组为2组每组为768//8维度，需要复制4次k-v，被8个头共享\n",
    "    # 假设x的每个token的维度为 hidden_dim，那么，head_dim = hidden_dim // num_heads，\n",
    "    def split_head(self, x, group_num=None):\n",
    "\n",
    "        batch_size,seq_len = x.size()[:2]\n",
    "\n",
    "        if group_num == None:\n",
    "            # x 的形状为 batch_size, seq_len, hidden_dim。其中hidden_dim = num_heads * head_dim\n",
    "            # 返回 batch_size,num_heads, seq_len, head_dim, 其中将输入的hidden_dim均分多头，\n",
    "            # 均分后的为head_dim 计算注意力\n",
    "            return x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        else:\n",
    "            # x为self.k-self.v处理后的 形状为 batch_size, seq_len, group_num*head_dim\n",
    "            # 将输入x重新 reshape 到指定形状并进行转置操作，以便符合后续处理所需的维度\n",
    "            x = x.view(batch_size, -1, group_num, self.head_dim).transpose(1,2)\n",
    "\n",
    "            # 通过维度扩展和重塑，调整x的形状，以适应多头注意力机制的需求\n",
    "            # `.expand()` 不创建新的数据副本，通过广播机制重复引用原始数据。通过广播机制共享内存中的k-v数据。\n",
    "            # 共享的 k-v相当于 参数了减少了 self.num_heads // group_num倍，实际计算的时候仍然使用复制的参数构造出self.num_heads个头\n",
    "            x = x[:, :, None, :, :].expand(batch_size, group_num, # 如果：group_num = 1 为 MQA，意味着q为多个头（假设为8），共享1组相同的k-v（8/1=8复制了8次）。 如果：group_num为num_head为 MHA，意味着q为多头，且k-v也为多头没有发生复制\n",
    "                                           self.num_heads // group_num, # 假如num_heads为8，group_num为2将k和v都分为2组，每组需要4个k-v来进行 GQA。\n",
    "                                           seq_len, self.head_dim).reshape # 由于expend复制了内存，导致，内存不连续，所以不能使用view改变形状\n",
    "            (batch_size, self.num_heads // group_num * group_num, seq_len, self.head_dim) # 因为，广播后内存就不会再变了，所以可以reshape不影响后续计算, 同一个k-v通过广播共享了 num_heads // group_num 次，其实参数全都共享1个k-v 能够的group_num个q的，复制了 多次\n",
    "\n",
    "            return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-07T05:00:28.191606600Z",
     "start_time": "2025-01-07T05:00:27.567276300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalGroupQueryAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    实现了一个基于组查询的因果注意力机制模块。\n",
    "\n",
    "    该模块通过将隐藏状态线性变换为查询、键和值，然后在多个注意力头上进行组查询，\n",
    "    最后通过一个线性变换整合结果，输出最终的注意力结果。特别地，该模块实现了因果注意力，\n",
    "    确保每个位置只能关注到它之前的位置。\n",
    "\n",
    "    参数:\n",
    "    hidden_size (int): 隐藏状态的维度。\n",
    "    head_num (int): 注意力头的数量。\n",
    "    group_num (int): 查询组的数量。\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, head_num, group_num):\n",
    "        super(CausalGroupQueryAttention, self).__init__()\n",
    "        # 初始化隐藏状态维度、注意力头数量和查询组数量\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_num = head_num\n",
    "        self.group_num = group_num\n",
    "        # 计算每个注意力头的维度\n",
    "        self.head_dim = self.hidden_size // head_num\n",
    "\n",
    "        # 初始化线性变换层，用于生成查询、键、值和最终的输出整合\n",
    "        self.q_linear = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = torch.nn.Linear(hidden_size, self.group_num * self.head_dim)\n",
    "        self.v_linear = torch.nn.Linear(hidden_size, self.group_num * self.head_dim)\n",
    "        self.o_linear = torch.nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_state, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数，实现了组查询因果注意力机制。\n",
    "\n",
    "        参数:\n",
    "        hidden_state (Tensor): 输入的隐藏状态，形状为[batch_size, seq_len, hidden_size]。\n",
    "        mask (Tensor, optional): 注意力掩码，用于指定某些位置的注意力得分为极小值，以避免在计算注意力时考虑这些位置。\n",
    "\n",
    "        返回:\n",
    "        Tensor: 注意力机制整合后的输出，形状为[batch_size, seq_len, hidden_size]。\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_state.size()\n",
    "\n",
    "        # 对隐藏状态进行线性变换，生成查询、键和值\n",
    "        query = self.q_linear(hidden_state)\n",
    "        key = self.k_linear(hidden_state)\n",
    "        value = self.v_linear(hidden_state)\n",
    "\n",
    "        # 将查询、键和值分割为多个注意力头\n",
    "        query = self.split_head(query)\n",
    "        key = self.split_head(key, self.group_num)\n",
    "        value = self.split_head(value, self.group_num)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        atten_score = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "\n",
    "        # 生成因果掩码，确保每个位置只能看到它之前的位置\n",
    "        causal_mask = torch.triu(torch.ones((seq_len, seq_len), device=hidden_state.device)).view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * -1e9\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :] + causal_mask\n",
    "        else:\n",
    "            mask = causal_mask\n",
    "\n",
    "        # 应用掩码atten_score的形状 batch_size, num_heads, seq_len, seq_len\n",
    "        # 这里mask通过广播机制，加到了atten_score上\n",
    "        atten_score += mask\n",
    "\n",
    "        # 计算注意力权重\n",
    "        atten_weights = torch.softmax(atten_score, dim=-1)\n",
    "\n",
    "        # 计算加权和\n",
    "        output = torch.matmul(atten_weights, value)\n",
    "        output = output.transpose(-1, -2).contiguous().view(batch_size, -1, self.group_num * self.head_dim)\n",
    "\n",
    "        return self.o_linear(output)\n",
    "\n",
    "    def split_head(self, x, group_num=None):\n",
    "        \"\"\"\n",
    "        将输入张量分割为多个注意力头。\n",
    "\n",
    "        参数:\n",
    "        x (Tensor): 输入张量，形状为[batch_size, seq_len, hidden_size]。\n",
    "        group_num (int, optional): 查询组的数量。\n",
    "\n",
    "        返回:\n",
    "        Tensor: 分割后的张量，形状为[batch_size, head_num, seq_len, head_dim]。\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.size()[:2]\n",
    "\n",
    "        if group_num is not None:\n",
    "            x = x.view(batch_size, -1, group_num, self.head_dim).transpose(1, 2)\n",
    "            x = x[:, :, None, :, :].expand(batch_size, group_num, self.head_num // group_num, seq_len, self.head_dim).reshape(batch_size, self.head_num, seq_len, self.head_dim)\n",
    "        else:\n",
    "            x = x.view(batch_size, -1, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T06:58:36.079678Z",
     "start_time": "2025-03-27T06:58:33.520401300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module() takes at most 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mGQA\u001B[39;00m(nn\u001B[38;5;241m.\u001B[39mmodules):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, groupnum, headnum, hiddensize):\n\u001B[0;32m      5\u001B[0m         \u001B[38;5;28msuper\u001B[39m(GQA, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[1;31mTypeError\u001B[0m: module() takes at most 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GQA(nn.modules):\n",
    "    def __init__(self, groupnum, headnum, hiddensize):\n",
    "        super(GQA, self).__init__()\n",
    "        self.headnum = headnum\n",
    "        self.groupnum = groupnum\n",
    "        self.hiddensize = hiddensize\n",
    "        self.headdim = self.hiddensize // self.headnum\n",
    "\n",
    "        self.qlinear = nn.Linear(hiddensize, hiddensize)\n",
    "        self.klinear = nn.Linear(hiddensize, groupnum*self.headdim)\n",
    "        self.vlinear = nn.Linear(hiddensize, groupnum*self.headdim)\n",
    "        self.olinear = nn.Linear(hiddensize,hiddensize)\n",
    "\n",
    "    def forward(self,x,mask=None):\n",
    "        batchsize = x.size()[0]\n",
    "\n",
    "        q = self.qlinear(x)\n",
    "        k = self.klinear(x)\n",
    "        v = self.vlinear(x)\n",
    "\n",
    "        q = self.split_head(q)\n",
    "        k = self.split_head(k,self.groupnum)\n",
    "        v = self.split_head(v,self.groupnum)\n",
    "\n",
    "        att_score = torch.matul(q,k.transpose(-1,-2)) // torch.sqrt(torch.tensor(self.headdim))\n",
    "        if mask:\n",
    "            att_score += mask * -1e-9\n",
    "\n",
    "        att_prob = torch.softmax(att_score, dim = -1)\n",
    "        output = torch.matmul(att_prob,v).transpose(1,2).contiguous().view(batchsize, -1, self.numhead*self.headdim)\n",
    "        output = self.olinear(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def split_head(self,x, group_num=None):\n",
    "        batchsize,seqlen = x.size()[:2]\n",
    "\n",
    "        if not group_num:\n",
    "            x = x.view(batchsize,-1,self.headnum,self.headdim).transpose(1,2)\n",
    "        else:\n",
    "            x = x.view(batchsize,-1,self.groupnum,self.headdim)\n",
    "            x = x[:,:,None:,:].extend(batchsize,self.groupnum,self.numhead//self.groupnum,seqlen,self.headdim).reshape(batchsize,self.headnum,seqlen,self.headdim)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-27T07:35:14.354945500Z",
     "start_time": "2025-03-27T07:35:14.096637400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
