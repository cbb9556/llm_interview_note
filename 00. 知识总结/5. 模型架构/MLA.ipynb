{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "参考：https://blog.csdn.net/baoyan2015/article/details/145497813\n",
    "\n",
    "https://chat.baidu.com/search?isShowHello=1&pd=csaitab&setype=csaitab&extParamsJson=%7B%22enter_type%22%3A%22ai_explore_home%22%7D&usedModel=%7B%22modelName%22%3A%22DeepSeek-R1%22%7D\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 128, 512]' is invalid for input of size 32768",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 85\u001B[0m\n\u001B[0;32m     83\u001B[0m mla \u001B[38;5;241m=\u001B[39m MultiHeadLatentAttention(d_model, num_heads, latent_rank\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n\u001B[0;32m     84\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(batch_size, seq_len, d_model)\n\u001B[1;32m---> 85\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmla\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28mprint\u001B[39m(output\u001B[38;5;241m.\u001B[39mshape)  \u001B[38;5;66;03m# [4, 128, 512]\u001B[39;00m\n",
      "File \u001B[1;32mD:\\devTools\\anaconda3\\envs\\VG-SVD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\devTools\\anaconda3\\envs\\VG-SVD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[1], line 71\u001B[0m, in \u001B[0;36mMultiHeadLatentAttention.forward\u001B[1;34m(self, x, mask)\u001B[0m\n\u001B[0;32m     69\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output \u001B[38;5;241m*\u001B[39m gate_weights\n\u001B[0;32m     70\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# [B, S, D/H]\u001B[39;00m\n\u001B[1;32m---> 71\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[43mattn_output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43md_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m# 输出投影\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_proj(attn_output)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: shape '[4, 128, 512]' is invalid for input of size 32768"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, latent_rank=32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.latent_rank = latent_rank\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # 低秩分解参数：键（Key）和值（Value）的分解矩阵 (原理2.1)\n",
    "        self.U_k = nn.Linear(d_model, latent_rank * num_heads, bias=False)\n",
    "        self.V_k = nn.Linear(latent_rank, self.head_dim, bias=False)\n",
    "        self.U_v = nn.Linear(d_model, latent_rank * num_heads, bias=False)\n",
    "        self.V_v = nn.Linear(latent_rank, self.head_dim, bias=False)\n",
    "\n",
    "        # 查询（Query）的常规投影\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # 动态融合门控权重生成 (原理2.3)\n",
    "        self.gate = nn.Linear(d_model, num_heads)\n",
    "\n",
    "        # 潜在偏置参数 (原理2.2中的L_i)\n",
    "        self.latent_bias = nn.Parameter(torch.zeros(num_heads, 1, 1))\n",
    "\n",
    "        # 输出投影\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # --- 低秩键/值投影 (原理2.1) ---\n",
    "        # 键投影：X -> U_k -> V_k\n",
    "        k_latent = self.U_k(x)  # [B, S, latent_rank * H]\n",
    "        k_latent = k_latent.view(batch_size, seq_len, self.num_heads, self.latent_rank)\n",
    "        k = self.V_k(k_latent)  # [B, S, H, D/H]\n",
    "\n",
    "        # 值投影：X -> U_v -> V_v\n",
    "        v_latent = self.U_v(x)\n",
    "        v_latent = v_latent.view(batch_size, seq_len, self.num_heads, self.latent_rank)\n",
    "        v = self.V_v(v_latent)\n",
    "\n",
    "        # --- 查询投影 ---\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 维度调整 (多头拆分)\n",
    "        q = q.permute(0, 2, 1, 3)  # [B, H, S, D/H]\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        # --- 注意力计算 (原理2.2) ---\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "        attn_scores = attn_scores + self.latent_bias  # 添加潜在偏置L_i\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)  # [B, H, S, D/H]\n",
    "\n",
    "        # --- 动态头融合 (原理2.3) ---\n",
    "        # 生成门控权重g_i\n",
    "        gate_weights = F.softmax(self.gate(x), dim=-1)  # [B, S, H]\n",
    "        gate_weights = gate_weights.permute(0, 2, 1).unsqueeze(-1)  # [B, H, S, 1]\n",
    "\n",
    "        # 加权融合\n",
    "        attn_output = attn_output * gate_weights\n",
    "        attn_output = attn_output.sum(dim=1)  # [B, S, D/H]\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # 输出投影\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# ---------- 使用示例 ----------\n",
    "if __name__ == \"__main__\":\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    seq_len = 128\n",
    "    batch_size = 4\n",
    "\n",
    "    mla = MultiHeadLatentAttention(d_model, num_heads, latent_rank=32)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    output = mla(x)\n",
    "    print(output.shape)  # [4, 128, 512]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T09:08:36.911354400Z",
     "start_time": "2025-03-22T09:08:25.850718600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "以下是基于PyTorch框架的MLA（Multi-head Latent Attention）简化版代码实现，包含‌低秩压缩‌和‌动态头融合‌核心逻辑。代码注释中标注了与原理对应的关键步骤。\n",
    "\n",
    "MLA 完整代码实现‌\n",
    "python\n",
    "Copy Code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, latent_rank=32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.latent_rank = latent_rank\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # 低秩分解参数：键（Key）和值（Value）的分解矩阵 (原理2.1)\n",
    "        self.U_k = nn.Linear(d_model, latent_rank * num_heads, bias=False)\n",
    "        self.V_k = nn.Linear(latent_rank, self.head_dim, bias=False)\n",
    "        self.U_v = nn.Linear(d_model, latent_rank * num_heads, bias=False)\n",
    "        self.V_v = nn.Linear(latent_rank, self.head_dim, bias=False)\n",
    "\n",
    "        # 查询（Query）的常规投影\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # 动态融合门控权重生成 (原理2.3)\n",
    "        self.gate = nn.Linear(d_model, num_heads)\n",
    "\n",
    "        # 潜在偏置参数 (原理2.2中的L_i)\n",
    "        self.latent_bias = nn.Parameter(torch.zeros(num_heads, 1, 1))\n",
    "\n",
    "        # 输出投影\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # --- 低秩键/值投影 (原理2.1) ---\n",
    "        # 键投影：X -> U_k -> V_k\n",
    "        k_latent = self.U_k(x)  # [B, S, latent_rank * H]\n",
    "        k_latent = k_latent.view(batch_size, seq_len, self.num_heads, self.latent_rank)\n",
    "        k = self.V_k(k_latent)  # [B, S, H, D/H]\n",
    "\n",
    "        # 值投影：X -> U_v -> V_v\n",
    "        v_latent = self.U_v(x)\n",
    "        v_latent = v_latent.view(batch_size, seq_len, self.num_heads, self.latent_rank)\n",
    "        v = self.V_v(v_latent)\n",
    "\n",
    "        # --- 查询投影 ---\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 维度调整 (多头拆分)\n",
    "        q = q.permute(0, 2, 1, 3)  # [B, H, S, D/H]\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        # --- 注意力计算 (原理2.2) ---\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "        attn_scores = attn_scores + self.latent_bias  # 添加潜在偏置L_i\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)  # [B, H, S, D/H]\n",
    "\n",
    "        # --- 动态头融合 (原理2.3) ---\n",
    "        # 生成门控权重g_i\n",
    "        gate_weights = F.softmax(self.gate(x), dim=-1)  # [B, S, H]\n",
    "        gate_weights = gate_weights.permute(0, 2, 1).unsqueeze(-1)  # [B, H, S, 1]\n",
    "\n",
    "        # 加权融合\n",
    "        attn_output = attn_output * gate_weights\n",
    "        attn_output = attn_output.sum(dim=1)  # [B, S, D/H]\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # 输出投影\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# ---------- 使用示例 ----------\n",
    "if __name__ == \"__main__\":\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    seq_len = 128\n",
    "    batch_size = 4\n",
    "\n",
    "    mla = MultiHeadLatentAttention(d_model, num_heads, latent_rank=32)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    output = mla(x)\n",
    "    print(output.shape)  # [4, 128, 512]\n",
    "\n",
    "关键代码解析‌\n",
    "\n",
    "低秩压缩‌（对应原理2.1）：\n",
    "\n",
    "键（U_k和V_k）与值（U_v和V_v）的投影被分解为两个低秩矩阵，例如：\n",
    "python\n",
    "Copy Code\n",
    "k_latent = self.U_k(x)  # 第一次低秩投影\n",
    "k = self.V_k(k_latent)  # 第二次恢复维度\n",
    "\n",
    "\n",
    "潜在偏置‌（对应原理2.2）：\n",
    "\n",
    "python\n",
    "Copy Code\n",
    "self.latent_bias = nn.Parameter(torch.zeros(num_heads, 1, 1))  # 每个头独立偏置\n",
    "attn_scores = attn_scores + self.latent_bias  # 动态调整注意力权重\n",
    "\n",
    "\n",
    "动态头融合‌（对应原理2.3）：\n",
    "\n",
    "python\n",
    "Copy Code\n",
    "gate_weights = F.softmax(self.gate(x), dim=-1)  # 输入自适应的门控权重\n",
    "attn_output = attn_output * gate_weights  # 按头加权\n",
    "\n",
    "性能优化技巧‌\n",
    "缓存中间结果‌：在推理时缓存 k_latent 和 v_latent，避免重复计算。\n",
    "参数共享‌：可将 U_k 和 U_v 共享同一权重矩阵以减少参数量。\n",
    "混合精度训练‌：使用 torch.cuda.amp 加速计算。\n",
    "\n",
    "该代码实现了MLA的核心逻辑，可直接替换标准Transformer中的多头注意力模块。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
