{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "参考：https://blog.csdn.net/baoyan2015/article/details/145497813\n",
    "\n",
    "https://chat.baidu.com/search?isShowHello=1&pd=csaitab&setype=csaitab&extParamsJson=%7B%22enter_type%22%3A%22ai_explore_home%22%7D&usedModel=%7B%22modelName%22%3A%22DeepSeek-R1%22%7D\n",
    "\n",
    "MLA只是，将原始的 Linear映射矩阵需要存储 768*768的参数量变为，768*rank + rank*768，将需要存储的参数量变少\n",
    "但是q-k需要计算的多头注意力并没有变化"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MLA(nn.Module):\n",
    "    def __init__(self,nums_head):\n",
    "        super(MLA, self).__init__()\n",
    "        self.nums_head = nums_head\n",
    "        self.wka = nn.Linear(token_dim, lora_k_rank)\n",
    "        self.wkb = nn.Linear(lora_k_rank, nums_head*(q_head_dim + q_rp_dim))\n",
    "        self.wkva = nn.Linear(token_dim, lora_kv_rank + k_rp_dim)\n",
    "        self.wkvb = nn.Linear(lora_kv_rank, nums_head*(k_head_dim+v_head_dim))\n",
    "        self.olinear = nn.Linear(token_dim, token_dim)\n",
    "\n",
    "    def forward(self, mask):\n",
    "        batch_size, seq_len = x.size()[:2]\n",
    "        q_rp = self.wqb(self.wqa(x))\n",
    "        q_rp = q_rp.view(batch_size,seq_len, self.nums_head, (q_head_dim + q_rp_dim))\n",
    "        q, rp = torch.split(q_rp, [q_head_dim, q_rp_dim],dim=-1)\n",
    "        rp = apply_rope(rp)\n",
    "        q = torch.cat([q,rp],dim = -1)\n",
    "\n",
    "        kv_rk = self.wkva(x)\n",
    "        kv, rk = torch.split(kv_rk, [low_rk_rank, k_rp_dim],dim=-1)\n",
    "        rk = apply_rope(rk)\n",
    "        kv = self.wkvb(rk)\n",
    "        kv = kv.view(batch_size, seq_len, nums_head, k_head_dim + v_head_dim)\n",
    "        k,v = torch.split(kv, [k_head_dim, v_head_dim], dim = -1)\n",
    "        k = torch.cat([k,rk],dim = -1)\n",
    "\n",
    "        attn = torch.matul(q.transpose(-1,-2),k) // torch.sqrt(torch.tensor(q_head_dim + q_rp_dim))\n",
    "\n",
    "        if mask:\n",
    "            attn += mask*-1e-9\n",
    "\n",
    "        attp = torch.softmax(attn,dim = -1)\n",
    "        attns = torch.matul(attp, v).transpose(1,2).contiguous().view(batch_size,seq_len,num_heads*v_head_dim)\n",
    "        output = self.olinear(attns)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## DeepSeek MLA源码\n",
    "class MLA(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Headed Attention Layer (MLA).\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): Dimensionality of the input features.\n",
    "        n_heads (int): Number of attention heads.\n",
    "        n_local_heads (int): Number of local attention heads for distributed systems.\n",
    "        q_lora_rank (int): Rank for low-rank query projection.\n",
    "        kv_lora_rank (int): Rank for low-rank key/value projection.\n",
    "        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.\n",
    "        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.\n",
    "        qk_head_dim (int): Total dimensionality of query/key projections.\n",
    "        v_head_dim (int): Dimensionality of value projections.\n",
    "        softmax_scale (float): Scaling factor for softmax in attention computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.dim = args.dim  # 输入特征的维度d\n",
    "        self.n_heads = args.n_heads # 128\n",
    "        self.n_local_heads = args.n_heads // world_size  # word_size = 1  进程数\n",
    "        self.q_lora_rank = args.q_lora_rank  # 低秩查询投影的秩   0表示不使用低秩 1536\n",
    "        self.kv_lora_rank = args.kv_lora_rank # 低秩键/值投影的秩 512\n",
    "        self.qk_nope_head_dim = args.qk_nope_head_dim # 128\n",
    "        self.qk_rope_head_dim = args.qk_rope_head_dim # 64\n",
    "        self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim  # 128+64\n",
    "        self.v_head_dim = args.v_head_dim # 128\n",
    "\n",
    "        if self.q_lora_rank == 0:\n",
    "            self.wq = ColumnParallelLinear(self.dim, self.n_heads * self.qk_head_dim)\n",
    "        else:\n",
    "            self.wq_a = Linear(self.dim, self.q_lora_rank) # q_lora_rank = 1536   7168*1536\n",
    "            self.q_norm = RMSNorm(self.q_lora_rank) # 1536\n",
    "            self.wq_b = ColumnParallelLinear(self.q_lora_rank, self.n_heads * self.qk_head_dim) # 1536 * 128*(128+64)\n",
    "        self.wkv_a = Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim) # 7168*(512+64)\n",
    "        self.kv_norm = RMSNorm(self.kv_lora_rank)  # 512\n",
    "        self.wkv_b = ColumnParallelLinear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim)) # 512 * 128*(128+128)\n",
    "        self.wo = RowParallelLinear(self.n_heads * self.v_head_dim, self.dim)  # 128*128 * 7168\n",
    "        self.softmax_scale = self.qk_head_dim ** -0.5\n",
    "        if args.max_seq_len > args.original_seq_len:\n",
    "            mscale = 0.1 * args.mscale * math.log(args.rope_factor) + 1.0\n",
    "            self.softmax_scale = self.softmax_scale * mscale * mscale\n",
    "\n",
    "        if attn_impl == \"naive\":\n",
    "            self.register_buffer(\"k_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.qk_head_dim), persistent=False)\n",
    "            self.register_buffer(\"v_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.v_head_dim), persistent=False)\n",
    "        else:\n",
    "            self.register_buffer(\"kv_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank), persistent=False)\n",
    "            self.register_buffer(\"pe_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.qk_rope_head_dim), persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Forward pass for the Multi-Headed Attention Layer (MLA).\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).\n",
    "            start_pos (int): Starting position in the sequence for caching.\n",
    "            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n",
    "            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with the same shape as the input.\n",
    "        \"\"\"\n",
    "        bsz, seqlen, _ = x.size()  # batch_size, seq_len, dim   1，2，7168\n",
    "        end_pos = start_pos + seqlen\n",
    "        if self.q_lora_rank == 0:\n",
    "            q = self.wq(x)\n",
    "        else:\n",
    "            q = self.wq_b(self.q_norm(self.wq_a(x)))  #  1，2，7168 -->  1，2，1536  --> 1，2，128*(128+64)  先降维，再升维\n",
    "        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim)  # 1，2，128，128+64\n",
    "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1) # 对最后一个维度进行切分 1，2，128，128 && 1，2，128，64\n",
    "        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n",
    "        kv = self.wkv_a(x)  # 1，2，7168 --> 1，2，512+64  （） # 直接降维\n",
    "        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)  # 1，2，512 && 1，2，64\n",
    "        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis) # 1，2，1，64\n",
    "        if attn_impl == \"naive\":\n",
    "            q = torch.cat([q_nope, q_pe], dim=-1) # 1，2，128，128+64\n",
    "            kv = self.wkv_b(self.kv_norm(kv)) # 1，2，512 --> 1，2，128*(128+128)  对kv进行生维\n",
    "            kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim)  # 1，2，128，128+128（516）\n",
    "            k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1) # 1，2，128，128 && 1，2，128，128\n",
    "            k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1) # 1，2，128，128 && 1，2，1，64  --> 1，2，128，128+64\n",
    "            self.k_cache[:bsz, start_pos:end_pos] = k\n",
    "            self.v_cache[:bsz, start_pos:end_pos] = v\n",
    "            scores = torch.einsum(\"bshd,bthd->bsht\", q, self.k_cache[:bsz, :end_pos]) * self.softmax_scale  # 1，2，128，128  * 1，2，128，128+64\n",
    "        else:\n",
    "            wkv_b = self.wkv_b.weight if self.wkv_b.scale is None else weight_dequant(self.wkv_b.weight, self.wkv_b.scale, block_size) # 512 * 128*(128+128)\n",
    "            wkv_b = wkv_b.view(self.n_local_heads, -1, self.kv_lora_rank) # 128, 128+128, 512\n",
    "            q_nope = torch.einsum(\"bshd,hdc->bshc\", q_nope, wkv_b[:, :self.qk_nope_head_dim]) # 1，2，128，128\n",
    "            self.kv_cache[:bsz, start_pos:end_pos] = self.kv_norm(kv) # 1，2，512 --> 1，2，512\n",
    "            self.pe_cache[:bsz, start_pos:end_pos] = k_pe.squeeze(2) # 1，2，1，64 --> 1，2，64\n",
    "            scores = (torch.einsum(\"bshc,btc->bsht\", q_nope, self.kv_cache[:bsz, :end_pos]) +\n",
    "                      torch.einsum(\"bshr,btr->bsht\", q_pe, self.pe_cache[:bsz, :end_pos])) * self.softmax_scale # 1，2，128，128  * 1，2，128，128+64\n",
    "        if mask is not None:\n",
    "            scores += mask.unsqueeze(1)\n",
    "        scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)\n",
    "        if attn_impl == \"naive\":\n",
    "            x = torch.einsum(\"bsht,bthd->bshd\", scores, self.v_cache[:bsz, :end_pos]) # 1，2，128，128\n",
    "        else:\n",
    "            x = torch.einsum(\"bsht,btc->bshc\", scores, self.kv_cache[:bsz, :end_pos])\n",
    "            x = torch.einsum(\"bshc,hdc->bshd\", x, wkv_b[:, -self.v_head_dim:])\n",
    "        x = self.wo(x.flatten(2)) # 1，2，128，128 --> 1，2，7168\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 128, 512]' is invalid for input of size 32768",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 85\u001B[0m\n\u001B[0;32m     83\u001B[0m mla \u001B[38;5;241m=\u001B[39m MultiHeadLatentAttention(d_model, num_heads, latent_rank\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n\u001B[0;32m     84\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(batch_size, seq_len, d_model)\n\u001B[1;32m---> 85\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmla\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;28mprint\u001B[39m(output\u001B[38;5;241m.\u001B[39mshape)  \u001B[38;5;66;03m# [4, 128, 512]\u001B[39;00m\n",
      "File \u001B[1;32mD:\\devTools\\anaconda3\\envs\\VG-SVD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\devTools\\anaconda3\\envs\\VG-SVD\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[1], line 71\u001B[0m, in \u001B[0;36mMultiHeadLatentAttention.forward\u001B[1;34m(self, x, mask)\u001B[0m\n\u001B[0;32m     69\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output \u001B[38;5;241m*\u001B[39m gate_weights\n\u001B[0;32m     70\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# [B, S, D/H]\u001B[39;00m\n\u001B[1;32m---> 71\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[43mattn_output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_len\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43md_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m# 输出投影\u001B[39;00m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_proj(attn_output)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: shape '[4, 128, 512]' is invalid for input of size 32768"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, latent_rank=32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.latent_rank = latent_rank\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # 低秩分解参数：键（Key）和值（Value）的分解矩阵 (原理2.1)\n",
    "        self.U_k = nn.Linear(d_model, latent_rank * num_heads, bias=False)\n",
    "        self.V_k = nn.Linear(latent_rank, self.head_dim, bias=False)\n",
    "        self.U_v = nn.Linear(d_model, latent_rank * num_heads, bias=False)\n",
    "        self.V_v = nn.Linear(latent_rank, self.head_dim, bias=False)\n",
    "\n",
    "        # 查询（Query）的常规投影\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # 动态融合门控权重生成 (原理2.3)\n",
    "        self.gate = nn.Linear(d_model, num_heads)\n",
    "\n",
    "        # 潜在偏置参数 (原理2.2中的L_i)\n",
    "        self.latent_bias = nn.Parameter(torch.zeros(num_heads, 1, 1))\n",
    "\n",
    "        # 输出投影\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # --- 低秩键/值投影 (原理2.1) ---\n",
    "        # 键投影：X -> U_k -> V_k\n",
    "        k_latent = self.U_k(x)  # [B, S, latent_rank * H]\n",
    "        k_latent = k_latent.view(batch_size, seq_len, self.num_heads, self.latent_rank)\n",
    "        k = self.V_k(k_latent)  # [B, S, H, D/H]\n",
    "\n",
    "        # 值投影：X -> U_v -> V_v\n",
    "        v_latent = self.U_v(x)\n",
    "        v_latent = v_latent.view(batch_size, seq_len, self.num_heads, self.latent_rank)\n",
    "        v = self.V_v(v_latent)\n",
    "\n",
    "        # --- 查询投影 ---\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 维度调整 (多头拆分)\n",
    "        q = q.permute(0, 2, 1, 3)  # [B, H, S, D/H]\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        # --- 注意力计算 (原理2.2) ---\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "        attn_scores = attn_scores + self.latent_bias  # 添加潜在偏置L_i\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)  # [B, H, S, D/H]\n",
    "\n",
    "        # --- 动态头融合 (原理2.3) ---\n",
    "        # 生成门控权重g_i\n",
    "        gate_weights = F.softmax(self.gate(x), dim=-1)  # [B, S, H]\n",
    "        gate_weights = gate_weights.permute(0, 2, 1).unsqueeze(-1)  # [B, H, S, 1]\n",
    "\n",
    "        # 加权融合\n",
    "        attn_output = attn_output * gate_weights\n",
    "        attn_output = attn_output.sum(dim=1)  # [B, S, D/H]\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # 输出投影\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# ---------- 使用示例 ----------\n",
    "if __name__ == \"__main__\":\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    seq_len = 128\n",
    "    batch_size = 4\n",
    "\n",
    "    mla = MultiHeadLatentAttention(d_model, num_heads, latent_rank=32)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    output = mla(x)\n",
    "    print(output.shape)  # [4, 128, 512]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-22T09:08:36.911354400Z",
     "start_time": "2025-03-22T09:08:25.850718600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "以下是基于PyTorch框架的MLA（Multi-head Latent Attention）简化版代码实现，包含‌低秩压缩‌和‌动态头融合‌核心逻辑。代码注释中标注了与原理对应的关键步骤。\n",
    "\n",
    "MLA 完整代码实现‌\n",
    "\n",
    "```pycon\n",
    "python\n",
    "Copy Code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, latent_rank=32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.latent_rank = latent_rank\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # 低秩分解参数：键（Key）和值（Value）的分解矩阵 (原理2.1)\n",
    "        self.U_k = nn.Linear(d_model, latent_rank * num_heads, bias=False)\n",
    "        self.V_k = nn.Linear(latent_rank, self.head_dim, bias=False)\n",
    "        self.U_v = nn.Linear(d_model, latent_rank * num_heads, bias=False)\n",
    "        self.V_v = nn.Linear(latent_rank, self.head_dim, bias=False)\n",
    "\n",
    "        # 查询（Query）的常规投影\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # 动态融合门控权重生成 (原理2.3)\n",
    "        self.gate = nn.Linear(d_model, num_heads)\n",
    "\n",
    "        # 潜在偏置参数 (原理2.2中的L_i)\n",
    "        self.latent_bias = nn.Parameter(torch.zeros(num_heads, 1, 1))\n",
    "\n",
    "        # 输出投影\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # --- 低秩键/值投影 (原理2.1) ---\n",
    "        # 键投影：X -> U_k -> V_k\n",
    "        k_latent = self.U_k(x)  # [B, S, latent_rank * H]\n",
    "        k_latent = k_latent.view(batch_size, seq_len, self.num_heads, self.latent_rank)\n",
    "        k = self.V_k(k_latent)  # [B, S, H, D/H]\n",
    "\n",
    "        # 值投影：X -> U_v -> V_v\n",
    "        v_latent = self.U_v(x)\n",
    "        v_latent = v_latent.view(batch_size, seq_len, self.num_heads, self.latent_rank)\n",
    "        v = self.V_v(v_latent)\n",
    "\n",
    "        # --- 查询投影 ---\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 维度调整 (多头拆分)\n",
    "        q = q.permute(0, 2, 1, 3)  # [B, H, S, D/H]\n",
    "        k = k.permute(0, 2, 1, 3)\n",
    "        v = v.permute(0, 2, 1, 3)\n",
    "\n",
    "        # --- 注意力计算 (原理2.2) ---\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "        attn_scores = attn_scores + self.latent_bias  # 添加潜在偏置L_i\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)  # [B, H, S, D/H]\n",
    "\n",
    "        # --- 动态头融合 (原理2.3) ---\n",
    "        # 生成门控权重g_i\n",
    "        gate_weights = F.softmax(self.gate(x), dim=-1)  # [B, S, H]\n",
    "        gate_weights = gate_weights.permute(0, 2, 1).unsqueeze(-1)  # [B, H, S, 1]\n",
    "\n",
    "        # 加权融合\n",
    "        attn_output = attn_output * gate_weights\n",
    "        attn_output = attn_output.sum(dim=1)  # [B, S, D/H]\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        # 输出投影\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# ---------- 使用示例 ----------\n",
    "if __name__ == \"__main__\":\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    seq_len = 128\n",
    "    batch_size = 4\n",
    "\n",
    "    mla = MultiHeadLatentAttention(d_model, num_heads, latent_rank=32)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    output = mla(x)\n",
    "    print(output.shape)  # [4, 128, 512]\n",
    "\n",
    "关键代码解析‌\n",
    "\n",
    "低秩压缩‌（对应原理2.1）：\n",
    "\n",
    "键（U_k和V_k）与值（U_v和V_v）的投影被分解为两个低秩矩阵，例如：\n",
    "python\n",
    "Copy Code\n",
    "k_latent = self.U_k(x)  # 第一次低秩投影\n",
    "k = self.V_k(k_latent)  # 第二次恢复维度\n",
    "\n",
    "\n",
    "潜在偏置‌（对应原理2.2）：\n",
    "\n",
    "python\n",
    "Copy Code\n",
    "self.latent_bias = nn.Parameter(torch.zeros(num_heads, 1, 1))  # 每个头独立偏置\n",
    "attn_scores = attn_scores + self.latent_bias  # 动态调整注意力权重\n",
    "\n",
    "\n",
    "动态头融合‌（对应原理2.3）：\n",
    "\n",
    "python\n",
    "Copy Code\n",
    "gate_weights = F.softmax(self.gate(x), dim=-1)  # 输入自适应的门控权重\n",
    "attn_output = attn_output * gate_weights  # 按头加权\n",
    "\n",
    "性能优化技巧‌\n",
    "缓存中间结果‌：在推理时缓存 k_latent 和 v_latent，避免重复计算。\n",
    "参数共享‌：可将 U_k 和 U_v 共享同一权重矩阵以减少参数量。\n",
    "混合精度训练‌：使用 torch.cuda.amp 加速计算。\n",
    "\n",
    "该代码实现了MLA的核心逻辑，可直接替换标准Transformer中的多头注意力模块。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
