[Scia Reto](https://sciareto.org) mind map   
> __version__=`1.1`,showJumps=`true`
---

# 70\-大模型的幻觉问题篇
> mmd.emoticon=`tree`


## 解决重复性问题

### 会导致幻觉问题

### 幻觉也就是胡说八道的问题，  解决重复性问题， 使用top\-k就可能导致胡说八道

## 一、什么是 大模型幻觉问题？

### 1\.1 大模型幻觉问题定义

#### 当模型生成的文本不遵循原文（Faithfulness）或者<br/>不符合事实（Factualness），<br/>我们就可以认为模型出现了幻觉的问题

### 1\.2 何为 Faithfulness and Factualness？

#### • Faithfulness：是否遵循input content；

#### • Factualness：是否符合世界知识；

### 1\.3 针对不同任务，幻觉的定义有何差异？

#### 1\. 数据源（source）不一致问题

##### 摘要的数据源是document，data\-to\-text的数据源是data table，对话的数据源是对话历史

##### 而开放域对话的<br/>数据源可以是世界知识；

#### 1\. 容忍幻觉的程度不一致问题

##### 在摘要、data\-to\-text任务中，非常看重response的Faithfulness，因此这些任务对幻觉的容忍程度很低；

##### 而像开发域对话任务中，只需要response符合事实即可，容忍程度较高；

### 1\.4 传统任务中的模型幻觉 vs LLMs 中模型幻觉

#### • 在传统任务里，幻觉大都是指的是Faithfulness：

##### • Intrinsic Hallucination（信息冲突）: LMs在生成回复时，<br/>与输入信息产生了冲突，例如摘要问题<br/>里，abstract和document的信息不一致；

#### • 而面向LLMs，我们通常考虑的幻觉则是Factualness：

##### • Extrinsic Hallucination（无中生有）: LMs在生成回复时，输出一些并没有体现在输入中的额外信<br/>息，比如邮箱地址、电话号码、住址，并且难以验证其真假。（PS: 按照此定义，Extrinsic<br/>Hallucination有可能是真的信息，只是需要外部信息源进行认证）

## 二、为什么 会 出现 大模型幻觉问题？

### 2\.1 从 数据角度 进行分析

#### 1\. 训练数据可信度问题

#### 2\. 重复数据问题。

### 2\.2 从 模型角度 进行分析

#### • 模型结构：如果是较弱的backbone（比如RNN）可能导致比较严重的幻觉问题，但在LLMs时代应该不太可<br/>能存在这一问题；

#### • 解码算法：研究表明，如果使用不确定性较高的采样算法（e\.g\.，top\-p）会诱导LMs出现更严重的幻觉问<br/>题。甚至可以故意在解码算法中加入一些随机性，进一步让LMs胡编乱造（可以用该方法生成一些negative

#### • 暴露偏差：训练和测试阶段不匹配的exposure bias问题可能导致LLMs出现幻觉，特别是生成long\-form<br/>response的时候。

#### • 参数知识：LMs在预训练阶段记忆的错误的知识，将会严重导致幻觉问题。

## 三、如何 评估 大模型幻觉问题？

### 3\.1 Reference\-based

#### 1\. 基于Source Information和Target Reference：

##### 利用一些统计学指标，比如ROUGE、BLEU来评估输出结<br/>果和Source/Target信息的重叠度;

#### 2\. 基于Source Information：

##### 由于NLG任务里，Target输出往往是多种多样的，因此许多工作只基于Source信<br/>息进行幻觉的评估。比如Knowledge F1

#### 基于Reference的评价指标，基本上只能评价Faithfulness，<br/>而无法评价Factualness，因此通常不适用于LLMs。

### 3\.2 Reference\-Free

#### 3\.2\.1 基于IE

##### 将知识限定于可以用三元组形式表示的关系和事件，<br/>基于额外的IE模型进行抽取，接着使用额外模型进行验证；

##### • 缺点：

###### • 可能存在IE模型的错误传播问题；<br/>• 知识被限定在三元组形式。

#### 3\.2\.2 基于QA

##### • 介绍：

###### 1\. 第一步先基于LM生成的回复，<br/>使用一个QG\(question generation\)模型生成一系列QA pairs；

###### 2\. 第二步给定Source Information，<br/>让QA模型对上一步生成的Question进行回复；

###### 3\. 第三步则是通过对比第一步的answers和第二步的answers，<br/>计算匹配指标，衡量模型的幻觉问题；

##### • 缺点：

###### • 可能存在IE模型的错误传播问题；

###### • 难以评估Factualness，因为上述第二步里面，Source Information<br/>不可能包含全部的世界知识，因此<br/>对于一些问题难以生成可靠的回复

#### 3\.2\.3 基于NLI

##### • 介绍：

###### 基于NLI的方法通过NLI模型评估是否Source Information<br/>可以蕴含Generated Text，从而评估是否出<br/>现了幻觉现象。

##### • 缺点：

###### 1\. Off\-the\-shelf NLI模型用于核查事实效果不是很好

###### 2\. 无法评估需要世界知识的幻觉问题：仅能依赖于Source进行核查；

###### 3\. 都是sentence\-level的，无法支撑更细粒度的幻觉检查；

#### 3\.2\.4 基于Factualness<br/> Classification Metric

##### • 介绍：标注/构造一批和幻觉/事实有关的数据，训练检测模型，<br/>利用该模型评估新生成文本的幻觉/事实问题。

#### 3\.2\.5 人工评估

##### • 介绍：目前为止最靠谱的，此外还可以依靠LLM打分

##### 比如利用GPT4，但是GPT4也存在着严重的幻觉问<br/>题，即使经过retrival\-augment，检索回来的信息也有可能是错误的）

## 四、如何 缓解 大模型幻觉问题？

### 4\.1 基于数据的工作

#### 4\.1\.1 构建高质量数据集

##### 1\. 人工标注

##### 1\. 自动筛选：

### 4\.2 模型层面的工作

#### 4\.2\.1 模型结构

##### • 检索增强

###### 被证明可以显著减少幻觉问题，e\.g\., LLaMA\-index。

##### • 在解码时减少模型的生成随机性

#### 4\.2\.2 训练方式

##### • 可控文本生成：将幻觉的程度作为一个可控的属性，利用可控文本生成技术进行控制

##### • 提前规划骨架，再生成：sketch to content

##### • 强化学习：

##### • 多任务学习: 通过设计合适的额外任务，可以达到减轻幻觉的效果。

##### • 后处理：设计一个小模型专门用于fix幻觉错误。

### 4\.3 可能的后续方向

#### 1\. 更细粒度的幻觉评估：

#### 2\. 知识的定义和诱导：

#### 3\. 幻觉消除：

##### a\. 检索增强：互联网/外挂知识库\(LLaMA Index\)<br/>b\. 强化学习（RLHF）<br/>c\. 知识诱导/注入<br/>d\. 直接修改LLM中错误记忆的知识：Model Editing工作，如ROME，MEMIT等
> align=`left`

