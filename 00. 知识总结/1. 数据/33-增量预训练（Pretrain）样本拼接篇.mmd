[Scia Reto](https://sciareto.org) mind map   
> __version__=`1.1`,showJumps=`true`
---

# 33\-增量预训练（Pretrain）样本拼接篇
> mmd.emoticon=`tree`


## 增量预训练（Pretrain）样本拼接篇

### Pretrain阶段，为什么需要拼接

#### 为了提高pretrain效率、拓展LLM最大长度，随机将若干条短文本进行拼接是pretrain阶段常见手<br/>段。

### 有哪些 拼接方式

#### 拼接方式一：Random Concatenate

##### 随机将短文本 \{examples\_i\} 拼接成 \{examples\_k\} 以打满maxLen是pretrain的常见手段

##### 能够降低padding占比、提高训练效率，还能使LLM具备更好的长文本处理能力

##### 问题

###### 绝大多数情况下构成 Example 的多个 examples 彼此互不相关，无法提供有效的上<br/>下文信息

###### LLM自然也无法从拓宽的窗口中获得反馈

###### 在语料较少、分布比较集中时，LLM<br/>很有可能从多次、偶然的（因拼接导致的）噪音共现中拟合到错误的特征

###### 如果语料足够<br/>多、分布足够广，LLM仍能通过足够的contrastive，逐渐聚焦于 examples 本身而非其他无关 <br/>examples 。

#### 拼接方式二：Random Concatenate \+ NoiseMask

##### 为缓解上述的无关 examples 间的噪音共现问题

##### 尝试过添加自定义attentionMask，使<br/>LLM在pretrain 时仅 focus on 当前 example

##### 该方法在ICL few\-shot上相比2\.1（也<br/>即常规pretrain方法）有1\.6%左右的提升

##### 问题

###### 相对位置编码（如ALIBI、ROPE）的token\-wise相对位置信息会在<br/>attentionScore矩阵对应位置有所体现

###### 施加了attentionMask，这部分相对位置信息经过<br/>softmax会被完全掩盖/误杀，也即LLM无法在BP过程中，从跨 examples 间获得反馈（不论是相对<br/>位置的反馈还是语义信息的反馈）

###### 因此在不考虑外推性的前提下，这种pretrain方法仍是在短文<br/>本窗口内进行训练，没有真正意义上实现maxLen级别的长文本训练，只能起到提高训练效率的作<br/>用。

####  拼接方式三：Random Concatenate \+ Cluster

##### 能否既不施加attentionMask，也能让LLM不受跨 examples 干扰甚至还能获<br/>益的方法呢？

##### 以实体、语义等维度对 \{examples\_i\} 进行聚类，<br/>使构成同一个 <br/>Example

###### examples 存在真实可靠的信息共现前提，<br/>从而LLM更加不容易从噪音共现中学偏

###### 也能从pretrain中适应更加广泛全局的attention、从而具备更好的长文本处理能力。

##### 问题

###### 沿实体维度进行聚类

####### 比较棘手的问题：信息重复

####### 以及经过关键词、<br/>语义去重后仍难以避免的信息泄露

###### 基于语义对 examples 进行聚合、拼接，因<br/>此笔者开始时也十分好奇作者如何妥善处理泄露问题。

#### 拼接方式四：IN\-CONTEXT PRETRAINING

##### 在拼接时，利用语义相似度，优先将最相似的<br/>进行拼接，从而构成语义更加连贯流畅的上下文

###### 1\. 将 \{examples\_i\} embedding化（作者使用了contriever）

###### 2\. 基于余弦距离进行数据去重；

###### 3\. 基于旅行商思想，不断串联最相关的 examples（每个 example 用完即扔，不会repeat）；

###### 4\. 基于拼接后的 \{examples\_k\} 进行pretrain；

##### 文中多次强调了数据去重的重要性，并经过消融实验验证了去重对ICLM的正向增益

###### 分布更广泛的<br/>数据、更妥善的去重操作，或许也是ICLM能够有效的重要原因。

##### 相比实体，沿语义聚合的 \{examples\_i\} 分布更加平缓，

###### 受泄露影响的风险更低；
